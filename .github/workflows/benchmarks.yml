name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly on Saturdays at 3 AM UTC
    - cron: '0 3 * * 6'

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark memory-profiler

    - name: Install package
      run: |
        pip install -e .

    - name: Run simulation benchmarks
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        python -c "
        import time
        import psutil
        import os
        from eles_core.engine import SimulationEngine

        print('=== ELES Performance Benchmarks ===')

        # Memory usage before
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024 / 1024  # MB

        # Basic simulation benchmark
        start_time = time.time()
        try:
            engine = SimulationEngine()
            # Run a sample simulation if method exists
            if hasattr(engine, 'run_simulation'):
                result = engine.run_simulation()
        except Exception as e:
            print(f'Simulation benchmark skipped: {e}')

        end_time = time.time()
        duration = end_time - start_time

        # Memory usage after
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = memory_after - memory_before

        print(f'Simulation Duration: {duration:.2f} seconds')
        print(f'Memory Used: {memory_used:.2f} MB')
        print(f'Peak Memory: {memory_after:.2f} MB')
        "

    - name: Run visualization benchmarks
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        export MPLBACKEND=Agg
        python -c "
        import time
        import matplotlib.pyplot as plt
        from visualizations.charts import create_impact_timeline, create_probability_distribution

        print('=== Visualization Performance Benchmarks ===')

        # Generate sample data
        import numpy as np
        time_data = list(range(100))
        impact_data = np.random.random(100) * 100

        # Timeline chart benchmark
        start_time = time.time()
        fig = create_impact_timeline(time_data, impact_data, 'Benchmark Timeline')
        plt.close(fig)
        timeline_duration = time.time() - start_time

        # Distribution chart benchmark
        start_time = time.time()
        distribution_data = np.random.normal(50, 15, 1000)
        fig = create_probability_distribution(distribution_data, 'Benchmark Distribution')
        plt.close(fig)
        distribution_duration = time.time() - start_time

        print(f'Timeline Chart: {timeline_duration:.3f} seconds')
        print(f'Distribution Chart: {distribution_duration:.3f} seconds')
        "

    - name: Generate benchmark report
      run: |
        echo "# Performance Benchmark Report" > benchmark-report.md
        echo "" >> benchmark-report.md
        echo "Generated on: $(date)" >> benchmark-report.md
        echo "Commit: ${{ github.sha }}" >> benchmark-report.md
        echo "" >> benchmark-report.md
        echo "## System Information" >> benchmark-report.md
        echo "- OS: $(uname -a)" >> benchmark-report.md
        echo "- Python: $(python --version)" >> benchmark-report.md
        echo "- Memory: $(free -h | grep Mem | awk '{print $2}')" >> benchmark-report.md
        echo "- CPU: $(nproc) cores" >> benchmark-report.md

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          benchmark-report.md
